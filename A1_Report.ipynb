{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Copy of A1_ReportDraft.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amy12564896/UTS_ML2019_ID12564896/blob/master/A1_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeYZlpLJiBgq",
        "colab_type": "text"
      },
      "source": [
        "# Draft and Experiment Area"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUiG5nrwiBgw",
        "colab_type": "text"
      },
      "source": [
        "1. First impression\n",
        "    * What is my chosen paper to read?\n",
        "    * What type of the main contribution the paper has made?\n",
        "        - A theory or proposition (revealing something, from unknown to known)\n",
        "        - A method or algorithm (inventing a technique, from undoable to doable)\n",
        "\n",
        "    * _Before_ reading the main body of the paper, write down your first impression  obtained from its abstract and short introduction.\n",
        "    * Why does the paper attract you, such as, How it surprised you? Why do you think it addresses an important topic that will be helpful in your future study of machine learning?\n",
        "    \n",
        "2. Read the paper abstract and introduction, list here all the notions that you don't know the precise meaning. If you think you have completed your list,  compare the list with people around you who have chosen the same or a similar paper.\n",
        "\n",
        "3. (During the next 7 days) Re-consider the central problem of the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXAE8ESxucsf",
        "colab_type": "text"
      },
      "source": [
        "This research aims to solve the problem for when lebelling a persons identity in a training set they can find them again in a test set under different conditions. It involves investigating facial recognition algorithms in relation to thier ability to recognise the same face with different expressions varying light sources/strengths. \n",
        "Using previously explored methods such as Correlation, Eigenfaces and Linear Subspaces to compare with their suggested algorith FIsherFaces. \n",
        "Fisherfaces maximises ratio of between-class scatter to that of within-class scatter. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftPO2aHv-VU9",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "*   New algorithm/method\n",
        "*   Comparison of method\n",
        "\n",
        "\n",
        "\n",
        "Fisherface - Fisher's linear discriminant is a classical technique in paern recognition. \n",
        "Eiganface based on linearly projecting the image space to a low dimentional feature space. it users princpal component analysis (PCA) for dimensionality reduction, yields rojection directions that maximise total scatter across all classes - all images of all faces. in chosing the projection which maximises the total scatter, PCA retains unwanted variations due to lighting and facial expression. \n",
        "\n",
        "\n",
        "*    Correlation - nearest neighbour classifier. the image in the test set is recognised (classified) by assigning it a label ofthe closest point inthe learningset where distances are measured in the image space. if all of the iamges are normalised to have zero mean and unit variane, then this procedure is equivalent to choosing the image in the learning set that best correlates with the test image. \n",
        "  *    Disadvantages - images in learning set and test set are gathered under varying lighting conditions, then the corresponding points in the ima ge space may not be tightly clusted, therefore the learning set would need to be densely sampled under manylighting conditions computationallyxeive, must test every face in the learning set against the test set. Therefore also requring lots of storage. \n",
        "*   Eigenfaces - Coose a dimensionality reducing linear projection that maximises scatter of all projected samples. \n",
        "  *   Disadvantages - scatter being maximised is due not only to the between class scatter that is useful for classification but also to the within class scatter that for classification purposes is unwanted information. oints in the projected space will not be wel clustered and worse, classes may be smeared together when the illumination changes.\n",
        "* Linear Subspaces - FOr each face, use three or more images taken under different lighting directions to construct a 3D basis for the linear subspace. Three basis vectors have the same dimensionality as the training images and can be thought of as basis images. to perform recognition, we simply compute the distance of a new image to each linear subspace and choose the face corresponding to the shortest distance. This method is a variant of the photometric alignment method. If there is no noise or shadowing it will achieve error free classification under ay lighting conditions provided the surfaces obey Lambertian reflectance model. \n",
        "  *   Disadvantages - due to self-shadowing, specularities and facial expressions, some regions inimages of the face have variabliity which doesn't agree with the linear subspace model .given enough images of faces we can learn which regions are good/bad for recognition. o recognise the test image, we must measure the distance to the linear subspace for each person, it needs  large number of images to represent the variability of each class and computationally expensive. Storage msut keep 3 images of each person. \n",
        "*   Fisherfaces - Class specific method, in the sense that it tries to \"shape\" the scatter in order to make it more reliable for classification. Ratio between-class scatter and within-class scatter is maximised. Projecting the image set to a lower dimensional sace so that the resulting within-class scatter matrix is nonsingular. \n",
        "\n",
        "\n",
        "                \n",
        "        \n",
        "                \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz-fqBqQEMC9",
        "colab_type": "text"
      },
      "source": [
        "Three sections of experimentation \n",
        "- Variation in lighting\n",
        "- Variation Expression, eye wear and lighting\n",
        "- Glasses Recognition \n",
        "\n",
        "Used two different databases \n",
        "- Harvard Robotics Laboratory\n",
        "- Constructed a database at Yale. \n",
        "\n",
        "Experiment 1 \n",
        "- Created 5 subsets from Harvard. \n",
        "- Each subset had different numbers of images, with different lightsources.\n",
        "- Classification was pperformed using a nearest neghbour classifier. \n",
        "- all training images of an individual were projected into features space\n",
        "- Images were cropped to show the face without the contour of the head\n",
        "- Exptrapolation - each method was trained on samples from subset 1 and tested using sambles from subset 1,2,3\n",
        "- Intropoloation - trained on subset 1 and 5, tested on 2, 3, 4\n",
        "\n",
        "Results\n",
        "- Shows results from subsets 1,2,3 (why not include 4,5)\n",
        "- Error rates are shown for each method under different subsets - with reduced space (which I don't totally get) \n",
        "- Are there other measures of success that one should be measuring these comparisons on \n",
        "- They talk about other metrics of success - i.e. computation time, but only against a couple of them, if this is important to a decision they should compare it on all of them?\n",
        "- Intropolation is surely a better measure - why do extraporlation at all, of course there's going to be a massively lower error rate for the first one... Interpolation (spelling oops) has higher error rates I think. I think the extraporlation result is confusing... \n",
        "- \n",
        "\n",
        "\n",
        "Experiment 2 \n",
        "- USing YALE Center database\n",
        "- only 16 subjects, ten iamges from each, male and female, some wore glasses. \n",
        "- Each image has different conditions - lighting, glasses, expressions\n",
        "- Some of the images were closely cropped, ohers wer full face with background and head shape included. \n",
        "- Error rates were determined by the 'leaving one out' strategy - where to classify an image of a erson that image was removed from the dataset and the dimensionality reduction matrix was cmputed, all images in the database excluding the test image are then projected down into the reduced space to be used for classification and then nearest neighbour was done. \n",
        "\n",
        "Results\n",
        "- Can see that full face helps a lot \n",
        "- Fisherface performes well over all num ber of principle components... whatever that means\n",
        "- The reduced spaces are all different - does that change how we can compare these answers? or is that just to make each method work?\n",
        "- Linear subspace worse in this one - but it's a different data set? How does that work? There are other variables than just light going on ... it's gone from just light changing to face expression, light and facial wear changing - shoudl probably do tests on each kind of change to see how it adapts, where exaclty its' weakness is?\n",
        "\n",
        "\n",
        "Experiment 3 \n",
        "- Two classes, glasses and no glasses\n",
        "- Recognition rates obtained by cross validation\n",
        "- Compares PCA to Fisherface only \n",
        "\n",
        "Results\n",
        "- PCA did way worse - has a higher reduced space? Need to find out what a reduced space is. \n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OsGK4wQOrkL",
        "colab_type": "text"
      },
      "source": [
        "Application domain - I suppose is Facial recognition - however it's really image/pattern recognition with relation to faces. Therefore it is appropriate as it is a growing field within AI - and has many potential uses although the ethics behind it are not fully clear. It could be applied for other image recognition, for finding weeds among native plants, dying coral vs healthy coral ... ?\n",
        "\n",
        "\n",
        "Suggestions - the work brings up some suggestions - large database, light conditions variations (controlled vs uncontrolled variation). Wouldn't hold up against EXTREME light conditions, equally too low lighting with shadow dominating the degrading of perofrmance is significant. \n",
        "\n",
        "My own - mood detection, more obscuring facial issues (pre-post injury, sunglasses), motion blur, more camera angles and different quality of images. \n",
        "\n",
        "I wish I understood it more deeply. I find it interesting that the proposed technique does outperform the others so well, considering how diverse the lighting situations were it is impressive the accuracy levels. Even the human eye would struggle.\n",
        "\n",
        "I also find the ethical considerations behind this work interesting - it's important to understand where the technical side is up to but the concern is this paper doesn't touch on ethics. This work is considerably ethical grey - improving these techniques is concerning if there is no even mention of the ethics behind this. Whilst I understand this is a technical report perhaps my issue is then with how these reports do not mandatorialy include an ethics section as it seems highly important when new technology is emerging that those developing it consider the ethical implications. \n",
        "I mean this was done in 1997 wow. \n",
        "Looking at China's use of this technology now I feel like ethics sould have been included...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9uOLuHa-9vo",
        "colab_type": "text"
      },
      "source": [
        "It was fairly easy to follow the authors train of thought. The headings were definitely clear and useful in their ability to divide up the paper to find specific sections. \n",
        "Whilst discussing the previously developed methods in depth, there was little longitudinal literature review in this space. Which would have been useful in the introduction as I can only imagine this technology is still fairly new. \n",
        "The writing style, until the technical explanations was succinct and clear. However, once it became technical, it was very hard to follow - without clear layout of what different symbols were representing, jumping between different methods which have different jargon it became much harder to decipher exactly what was being said. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5Jv62GZzvIP",
        "colab_type": "text"
      },
      "source": [
        "Definitions \n",
        "\n",
        "Fishers Linear Discriminant - The idea proposed by Fisher is to maximize a function that will give a large separation between the projected class means while also giving a small variance within each class, thereby minimizing the class overlap. \n",
        "In other words, FLD selects a projection that maximizes the class separation. To do that, it maximizes the ratio between the between-class variance to the within-class variance.In short, to project the data to a smaller dimension and to avoid class overlapping, FLD maintains 2 properties.\n",
        "\n",
        "* A large variance among the dataset classes.\n",
        "* A small variance within each of the dataset classes.\n",
        "\n",
        "* Fisher’s Linear Discriminant, in essence, is a technique for dimensionality reduction, not a discriminant. For binary classification, we can find an optimal threshold t and classify the data accordingly. For multiclass data, we can (1) model a class conditional distribution using a Gaussian. (2) Find the prior class probabilities P(Ck), and (3) use Bayes to find the posterior class probabilities p(Ck|x).\n",
        "\n",
        "* To find the optimal direction to project the input data, Fisher needs supervised data.\n",
        "\n",
        "* Given a dataset with D dimensions, we can project it down to at most D’ equals to D-1 dimensions.\n",
        "https://sthalles.github.io/fisher-linear-discriminant/\n",
        "\n",
        "---\n",
        "\n",
        "Eigenfaces refers to an appearance-based approach to face recognition that seeks to capture the variation in a collection of face images and use this information to encode and compare images of individual faces in a holistic (as opposed to a parts-based or feature-based) manner. Specifically, the eigenfaces are the principal components of a distribution of faces, or equivalently, the eigenvectors of the covariance matrix of the set of face images, where an image with N pixels is considered a point (or vector) in N-dimensional space. The Eigenface approach is considered by many to be the first working facial recognition technology, and it served as the basis for one of the top commercial face recognition technology products. Eigenfaces is still often considered as a baseline comparison method to demonstrate the minimum expected performance of such a system. The motivation of Eigenfaces is twofold:\n",
        "\n",
        "* Extract the relevant facial information, which may or may not be directly related to human intuition of face features such as the eyes, nose, and lips. One way to do so is to capture the statistical variation between face images.\n",
        "* Represent face images efficiently. To reduce the computation and space complexity, each face image can be represented using a small number of parameters. http://www.scholarpedia.org/article/Eigenfaces \n",
        "\n",
        "---\n",
        "\n",
        "Principal Component Analysis - Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
        "\n",
        "\n",
        "1.   Standardization - The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable.\n",
        "2.   Covariance Matrix computation - The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or in other words, to see if there is any relationship between them. Because sometimes, variables are highly correlated in such a way that they contain redundant information. So, in order to identify these correlations, we compute the covariance matrix. The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables.\n",
        "3. Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components - Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on.  Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. As there are as many principal components as there are variables in the data, principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance.\n",
        "4.  Feature vector - In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector.\n",
        "5. Recast the data along the principal components axes - the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). This can be done by multiplying the transpose of the original data set by the transpose of the feature vector. https://towardsdatascience.com/a-step-by-step-explanation-of-principal-component-analysis-b836fb9c97e2\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "When should I use PCA?\n",
        "1. Do you want to reduce the number of variables, but aren’t able to identify variables to completely remove from consideration?\n",
        "2. Do you want to ensure your variables are independent of one another?\n",
        "3. Are you comfortable making your independent variables less interpretable?\n",
        "\n",
        "Yes to all three? Use PCA - if no to no.3 don't use PCA. \n",
        "https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Coming to Nearest Neighbour Classifier, it is clear from the term itself that for every test image we compare it with every labeled trained data to predict its label or class, whatever we call it. This is kind of online operation as we are predicting the class by simply traversing through all pre-labeled data. https://towardsdatascience.com/nearest-neighbour-classifier-4ad15516873\n",
        "\n",
        "---\n",
        "\n",
        "A Lambertian surface for reflection is a surface that appears uniformly bright from all directions of view and reflects the entire incident light. Lambertian reflectance is the property exhibited by an ideal matte or diffusely reflecting surface. Lambertian surfaces are often referred to as ideal diffusion surfaces. https://www.azooptics.com/Article.aspx?ArticleID=790 \n",
        "\n",
        "---\n",
        "\n",
        "This chapter presents an overview of the state-of-the-art methods for three-dimensional (3D) shape reconstruction, starting with a tutorial on the basic understanding of the problem with emphasis on photometric stereo technology. Photometric stereo is a method for recovering local surface shape and albedo from a number of images captured under different illumination directions. The photometric stereo method is simple to implement. https://www.sciencedirect.com/science/article/pii/S1076567008014018 \n",
        "\n",
        "---\n",
        "\n",
        "The objective of linear subspace learning is to learn a linear transformation matrix A from a training data set\n",
        "\n",
        "https://pdfs.semanticscholar.org/1d37/33e1a4b9d3e5b95fb7cc135a0819cba90c22.pdf?_ga=2.134862774.613342574.1566011079-199433084.1566011079 \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Fisherface is one of the popular algorithms used in face recognition, and is widely\n",
        "believed to be superior to other techniques, such as eigenface because of the effort to maximize\n",
        "the separation between classes in the training process\n",
        "https://iopscience.iop.org/article/10.1088/1742-6596/1028/1/012119/pdf\n",
        "\n",
        "\n",
        "---\n",
        "Face recognition is a method of identifying or verifying the identity of an individual using their face. Face recognition systems can be used to identify people in photos, video, or in real-time. Law enforcement may also use mobile devices to identify people during police stops. \n",
        "\n",
        "But face recognition data can be prone to error, which can implicate people for crimes they haven’t committed. Facial recognition software is particularly bad at recognizing African Americans and other ethnic minorities, women, and young people, often misidentifying or failing to identify them, disparately impacting certain groups.\n",
        "\n",
        "Face recognition systems use computer algorithms to pick out specific, distinctive details about a person’s face. These details, such as distance between the eyes or shape of the chin, are then converted into a mathematical representation and compared to data on other faces collected in a face recognition database. The data about a particular face is often called a face template and is distinct from a photograph because it’s designed to only include certain details that can be used to distinguish one face from another. \n",
        "\n",
        "Some face recognition systems, instead of positively identifying an unknown person, are designed to calculate a probability match score between the unknown person and specific face templates stored in the database. These systems will offer up several potential matches, ranked in order of likelihood of correct identification, instead of just returning a single result. \n",
        "https://www.eff.org/pages/face-recognition "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C7Tu3RoiBgy",
        "colab_type": "text"
      },
      "source": [
        "# Review Report on \"PaperName\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVYKPxciBg1",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPkKbSeziBg2",
        "colab_type": "text"
      },
      "source": [
        "## Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFakmeI_iBg5",
        "colab_type": "text"
      },
      "source": [
        "The research is about .... A theorem has been proved stating ... / An algorithm of ... has been proposed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vWLBdbgiBg7",
        "colab_type": "text"
      },
      "source": [
        "## Innovation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqB-tyCbiBg9",
        "colab_type": "text"
      },
      "source": [
        "The background at the time of the work is that people understood the problem as .... The creative idea is ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE7Wml59iBg_",
        "colab_type": "text"
      },
      "source": [
        "## Technical quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP3mDbbKiBhB",
        "colab_type": "text"
      },
      "source": [
        "The technical development if of high/low quality. The authors supported their theory using ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_kEsNnXiBhC",
        "colab_type": "text"
      },
      "source": [
        "## Application and X-factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9S2IsA9iBhD",
        "colab_type": "text"
      },
      "source": [
        "I find the proposal in the paper promising. ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFztLFuTiBhF",
        "colab_type": "text"
      },
      "source": [
        "## Presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE3uErkqiBhH",
        "colab_type": "text"
      },
      "source": [
        "The overall strucutre is clear. I found reading is easy / difficult. The paper could have been more attractive if the authors had organised ... / provided ... "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z4G0IfZiBhJ",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "[SHA48][1]: Author, Title, Info\n",
        "\n",
        "[1]:https://google.com"
      ]
    }
  ]
}