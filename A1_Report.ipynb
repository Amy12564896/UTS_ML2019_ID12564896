{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Copy of A1_ReportDraft.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amy12564896/UTS_ML2019_ID12564896/blob/master/A1_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeYZlpLJiBgq",
        "colab_type": "text"
      },
      "source": [
        "# Draft and Experiment Area"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5Jv62GZzvIP",
        "colab_type": "text"
      },
      "source": [
        "Definitions \n",
        "\n",
        "Fishers Linear Discriminant - The idea proposed by Fisher is to maximize a function that will give a large separation between the projected class means while also giving a small variance within each class, thereby minimizing the class overlap. \n",
        "In other words, FLD selects a projection that maximizes the class separation. To do that, it maximizes the ratio between the between-class variance to the within-class variance.In short, to project the data to a smaller dimension and to avoid class overlapping, FLD maintains 2 properties.\n",
        "\n",
        "* A large variance among the dataset classes.\n",
        "* A small variance within each of the dataset classes.\n",
        "\n",
        "* Fisher’s Linear Discriminant, in essence, is a technique for dimensionality reduction, not a discriminant. For binary classification, we can find an optimal threshold t and classify the data accordingly. For multiclass data, we can (1) model a class conditional distribution using a Gaussian. (2) Find the prior class probabilities P(Ck), and (3) use Bayes to find the posterior class probabilities p(Ck|x).\n",
        "\n",
        "* To find the optimal direction to project the input data, Fisher needs supervised data.\n",
        "\n",
        "* Given a dataset with D dimensions, we can project it down to at most D’ equals to D-1 dimensions.\n",
        "https://sthalles.github.io/fisher-linear-discriminant/\n",
        "\n",
        "---\n",
        "\n",
        "Eigenfaces refers to an appearance-based approach to face recognition that seeks to capture the variation in a collection of face images and use this information to encode and compare images of individual faces in a holistic (as opposed to a parts-based or feature-based) manner. Specifically, the eigenfaces are the principal components of a distribution of faces, or equivalently, the eigenvectors of the covariance matrix of the set of face images, where an image with N pixels is considered a point (or vector) in N-dimensional space. The Eigenface approach is considered by many to be the first working facial recognition technology, and it served as the basis for one of the top commercial face recognition technology products. Eigenfaces is still often considered as a baseline comparison method to demonstrate the minimum expected performance of such a system. The motivation of Eigenfaces is twofold:\n",
        "\n",
        "* Extract the relevant facial information, which may or may not be directly related to human intuition of face features such as the eyes, nose, and lips. One way to do so is to capture the statistical variation between face images.\n",
        "* Represent face images efficiently. To reduce the computation and space complexity, each face image can be represented using a small number of parameters. http://www.scholarpedia.org/article/Eigenfaces \n",
        "\n",
        "---\n",
        "\n",
        "Principal Component Analysis - Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
        "\n",
        "\n",
        "1.   Standardization - The aim of this step is to standardize the range of the continuous initial variables so that each one of them contributes equally to the analysis Mathematically, this can be done by subtracting the mean and dividing by the standard deviation for each value of each variable.\n",
        "2.   Covariance Matrix computation - The aim of this step is to understand how the variables of the input data set are varying from the mean with respect to each other, or in other words, to see if there is any relationship between them. Because sometimes, variables are highly correlated in such a way that they contain redundant information. So, in order to identify these correlations, we compute the covariance matrix. The covariance matrix is a p × p symmetric matrix (where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables.\n",
        "3. Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components - Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components. So, the idea is 10-dimensional data gives you 10 principal components, but PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on.  Organizing information in principal components this way, will allow you to reduce dimensionality without losing much information, and this by discarding the components with low information and considering the remaining components as your new variables. As there are as many principal components as there are variables in the data, principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance.\n",
        "4.  Feature vector - In this step, what we do is, to choose whether to keep all these components or discard those of lesser significance (of low eigenvalues), and form with the remaining ones a matrix of vectors that we call Feature vector.\n",
        "5. Recast the data along the principal components axes - the aim is to use the feature vector formed using the eigenvectors of the covariance matrix, to reorient the data from the original axes to the ones represented by the principal components (hence the name Principal Components Analysis). This can be done by multiplying the transpose of the original data set by the transpose of the feature vector. https://towardsdatascience.com/a-step-by-step-explanation-of-principal-component-analysis-b836fb9c97e2\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "When should I use PCA?\n",
        "1. Do you want to reduce the number of variables, but aren’t able to identify variables to completely remove from consideration?\n",
        "2. Do you want to ensure your variables are independent of one another?\n",
        "3. Are you comfortable making your independent variables less interpretable?\n",
        "\n",
        "Yes to all three? Use PCA - if no to no.3 don't use PCA. \n",
        "https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Coming to Nearest Neighbour Classifier, it is clear from the term itself that for every test image we compare it with every labeled trained data to predict its label or class, whatever we call it. This is kind of online operation as we are predicting the class by simply traversing through all pre-labeled data. https://towardsdatascience.com/nearest-neighbour-classifier-4ad15516873\n",
        "\n",
        "---\n",
        "\n",
        "A Lambertian surface for reflection is a surface that appears uniformly bright from all directions of view and reflects the entire incident light. Lambertian reflectance is the property exhibited by an ideal matte or diffusely reflecting surface. Lambertian surfaces are often referred to as ideal diffusion surfaces. https://www.azooptics.com/Article.aspx?ArticleID=790 \n",
        "\n",
        "---\n",
        "\n",
        "This chapter presents an overview of the state-of-the-art methods for three-dimensional (3D) shape reconstruction, starting with a tutorial on the basic understanding of the problem with emphasis on photometric stereo technology. Photometric stereo is a method for recovering local surface shape and albedo from a number of images captured under different illumination directions. The photometric stereo method is simple to implement. https://www.sciencedirect.com/science/article/pii/S1076567008014018 \n",
        "\n",
        "---\n",
        "\n",
        "The objective of linear subspace learning is to learn a linear transformation matrix A from a training data set\n",
        "\n",
        "https://pdfs.semanticscholar.org/1d37/33e1a4b9d3e5b95fb7cc135a0819cba90c22.pdf?_ga=2.134862774.613342574.1566011079-199433084.1566011079 \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Fisherface is one of the popular algorithms used in face recognition, and is widely\n",
        "believed to be superior to other techniques, such as eigenface because of the effort to maximize\n",
        "the separation between classes in the training process\n",
        "https://iopscience.iop.org/article/10.1088/1742-6596/1028/1/012119/pdf\n",
        "\n",
        "\n",
        "---\n",
        "Face recognition is a method of identifying or verifying the identity of an individual using their face. Face recognition systems can be used to identify people in photos, video, or in real-time. Law enforcement may also use mobile devices to identify people during police stops. \n",
        "\n",
        "But face recognition data can be prone to error, which can implicate people for crimes they haven’t committed. Facial recognition software is particularly bad at recognizing African Americans and other ethnic minorities, women, and young people, often misidentifying or failing to identify them, disparately impacting certain groups.\n",
        "\n",
        "Face recognition systems use computer algorithms to pick out specific, distinctive details about a person’s face. These details, such as distance between the eyes or shape of the chin, are then converted into a mathematical representation and compared to data on other faces collected in a face recognition database. The data about a particular face is often called a face template and is distinct from a photograph because it’s designed to only include certain details that can be used to distinguish one face from another. \n",
        "\n",
        "Some face recognition systems, instead of positively identifying an unknown person, are designed to calculate a probability match score between the unknown person and specific face templates stored in the database. These systems will offer up several potential matches, ranked in order of likelihood of correct identification, instead of just returning a single result. \n",
        "https://www.eff.org/pages/face-recognition "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C7Tu3RoiBgy",
        "colab_type": "text"
      },
      "source": [
        "# Review Report on \"PaperName\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVYKPxciBg1",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPkKbSeziBg2",
        "colab_type": "text"
      },
      "source": [
        "## Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFakmeI_iBg5",
        "colab_type": "text"
      },
      "source": [
        "This paper investigates various methods for facial recognition with specific regard to how different algorithms perform in response to different lighting conditions and facial expressions. Expressions do not distort the key features too dramatically but at the time this paper was produced, it was still enough of an alteration to the facial structure of a subject that significant expression changes would lower accuracy of a recognition system. Regarding lighting variations, when examining general image and pattern recognition with strong lighting using an fixed image of a Lambertian surface - where the light reflects uniformly when under different light sources therefore should be ideal for recognition – it will yield unreliable results due to the shadowing that differ in each image. For facial recognition, the direction and intensity of lighting can dramatically change the shape and primary features of a face to the extent that two images of the same face can be almost unrecognisable for the human eye highlighting the importance of training computer vision to recognise these changes.   \n",
        "\n",
        "This is a major challenge for widespread use of facial recognition as the environment in which a face is found will not be identical each time. Facial expressions change frequently, with none look identical between people, whilst lighting can vary dramatically in the diverse environments facial recognition may be used. Therefore, overcoming these environmental elements such as lighting is critical for expanding applications. \n",
        "\n",
        "The paper works to resolve this through experimenting with the Fisherface algorithm - based off the classical technique in pattern recognition known as Fisher’s Linear Discriminant (FLD) – is a class specific method that works to shape the scatters produced into a lower dimensionality space for faster computational time with more reliable results. Fisherface is tested on two different datasets with different light sources and facial expressions, these results are compared against Correlation, Linear Subspaces and Eigenfaces approaches on the same datasets in the same conditions. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vWLBdbgiBg7",
        "colab_type": "text"
      },
      "source": [
        "## Innovation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqB-tyCbiBg9",
        "colab_type": "text"
      },
      "source": [
        "Belhumeur, Hespanha and Kriegman contribute a new algorithm in their development of Fisherface. It has been presented alongside a comparison of other methods with experimentation on correlation, eigenfaces and linear subspace algorithms to contrast their performance against the new Fisherface approach. \n",
        "\n",
        "The comparison between the different algorithms is not only through experimentation of datasets to evaluate the different error rates testing different lighting conditions and facial expressions rather including overarching appropriate applications for each technique whilst also the disadvantages. The layout of the paper allows for all previous methods to be outlined with their disadvantages explained to exhibit how Fisherface works to resolve these issues.  As Fisherfaces combats the poor clustering of results that may be too loosely clustered or smeared together and the high storage or computational demand. \n",
        "\n",
        "The new algorithm is a combination of Principal Component Analysis (PCA) and Fisher Linear Discriminant (FLD). The Fisher Linear Discriminant is a classical method that minimises class overlap through maximising the ratio between class variance to the within class variance. This works to lower the dimensionality whilst still preserving linear separability. As previously PCA methods have smeared classes together which removes linear separability in the projected spaces which increases error in classification. The ingenuity of the Fisherface method is to apply PCA to achieve a large total scatter and then apply a standard FLD to have greater between class scatter to simplify the classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE7Wml59iBg_",
        "colab_type": "text"
      },
      "source": [
        "## Technical quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP3mDbbKiBhB",
        "colab_type": "text"
      },
      "source": [
        "The paper performed three different experiments on two different datasets to evaluate the new algorithm against pre-existing methods using error rates. The first experiment uses the Harvard dataset and performs extrapolation and interpolation. Even though the paper lays out five different subsets to compare, the results only outline three. For the extrapolation test one of the subsets is the training set which is odd as there were obviously more subsets made however not presented in the final report, rather the training set was which makes all the methods look far more accurate than they really are. The interpolation was trained and tested on different subsets which gave a better indication of the results without confusing a reader. \n",
        "\n",
        "The second experiment used an error rate that the paper discloses as the ‘leaving one out’ strategy. Performed on a different dataset this experiment does not simply change one condition, rather shifts light, expression and how the image was taken into the test. The significant shift makes it hard to directly compare to the first experiment however this is done in the paper which could be misleading for how quality of the algorithms. Although this experiment is likely the more important one, as the higher variability between images should expose a definitive difference in quality between the algorithms. \n",
        "\n",
        "The experiments yielded that the newly proposed Fisherface algorithm outperformed the others in majority of circumstances. However not every subset was experimented on in the Harvard dataset, which does not fully explore Fisherface’s ability to perform in different extreme lighting environments. Doing more variations of lighting subsets from Harvard could result in trends that show a certain lighting source changes performance. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_kEsNnXiBhC",
        "colab_type": "text"
      },
      "source": [
        "## Application and X-factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9S2IsA9iBhD",
        "colab_type": "text"
      },
      "source": [
        "The Fisherfaces algorithm is appropriately applied in image recognition with specific focus on facial recognition. To extend the use of the algorithm research into video facial recognition that has lower quality images and blurred faces could allow for a wider variety of uses. The algorithm identifies the facial features and can re-identify them even when in vastly different lighting environments and expression changes, therefore other domains could be in environmental areas where it could recognise the same species amongst other plants or identify if a fauna/flora is healthy or not. There are many environmental applications that could benefits from such pattern recognition outlined in the Fisherfaces capabilities. \n",
        "\n",
        "This 22 year old seminal paper for Fisherfaces offers insight into how facial recognition has been developed and how quickly it has gone from still photos under very controlled conditions to in modern mobiles and laptops as a security measure. However the paper doesn’t discuss the ethical implications of the technology it is developing – which now over two decades on see the use by the Chinese Government is using facial recognition on a large scale for ethically grey reasons (Mozur 2019). Belhumeur, Hespanha and Kriegman should have discussed the ethics of their work as it is a critical part of technology development to consider the potential unethical uses and work to mitigate them. However at the speed this technology is moving it is difficult, with newer algorithms evolving that are effective in real time such as the Viola-Jones method that uses neural networks (Kharkovyna 2019). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFztLFuTiBhF",
        "colab_type": "text"
      },
      "source": [
        "## Presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE3uErkqiBhH",
        "colab_type": "text"
      },
      "source": [
        "The paper was clear and succinct in its comparison of techniques and introduction of the Fisherface algorithm. The headings were clear in following the thought processes of the authors to understand how the problem was identified and approached. The previously developed methods were explained with disadvantages highlighted; however, the paper could have explored greater breadth of context that may have assisted in understanding some concepts that were brought up when discussing the different algorithms. Rather I had to research these acronyms to make sense of them – the paper was very much written for other academics in the same field. This is only reinforced as the paper became more technical in its descriptions of the algorithms and tests. The methods used for testing and errors were not clear and when discussing results, the structure that had been useful in following in the first part of the paper felt lost and it became jumbled between different methods performances.\n",
        "\n",
        "The paper could have been improved by including footnotes that explained some concepts that may have been new to a reader therefore making it easier to digest the high level of technical information being put forward. Additionally, organising the results section differently to discuss how each method performed and potential reasons for this would have allowed for greater clarity and consistency in comparison of the methods rather than simply not mentioning some of the methods. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z4G0IfZiBhJ",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "[SHA48][1]: Author, Title, Info\n",
        "\n",
        "[1]:https://google.com"
      ]
    }
  ]
}